# ü§ñ LLM Integration - Dokumentation

## √úbersicht

Diese Dokumentation beschreibt die Integration des Gemini LLM in den Telefon-Agenten f√ºr automatische Terminbuchungen.

## üìã Aktueller Status

### ‚úÖ Implementiert:
- `AppointmentAgent` Klasse mit Gemini LLM
- Kontext-aware Prompts
- Conversation History Management
- Fallback-Mechanismen
- Test-Funktion

### ‚ö†Ô∏è Noch zu implementieren:
- Integration in `call_server.py`
- Request-Daten √úbergabe via Twilio
- Supabase Integration f√ºr Request-Daten
- Summary zur√ºck an Frontend

## üèóÔ∏è Architektur

### Workflow:

```
Frontend (New Request)
    ‚Üì
Backend (FastAPI) - POST /api/process-request
    ‚Üì
start_call() mit request_id
    ‚Üì
Twilio Call ‚Üí webhook_url?request_id=xxx
    ‚Üì
Flask Server - /start_conversation
    ‚Üì
AppointmentAgent.set_context(request_data)
    ‚Üì
AppointmentAgent.get_response() ‚Üí Gemini LLM
    ‚Üì
TTS (ElevenLabs) ‚Üí Audio
    ‚Üì
Twilio spielt Audio ab
    ‚Üì
User spricht
    ‚Üì
/gather ‚Üí AppointmentAgent.get_response(user_input)
    ‚Üì
... Konversation continues ...
    ‚Üì
get_conversation_summary() ‚Üí Zur√ºck an Frontend
```

## üìù AppointmentAgent Klasse

### Initialisierung:

```python
from llmcall_method.agent import AppointmentAgent

agent = AppointmentAgent()
```

### Kontext setzen:

```python
request_data = {
    'title': 'Arzttermin vereinbaren',
    'description': 'Allgemeine Untersuchung',
    'preferred_time': 'n√§chste Woche vormittags',
    'user_profile': {
        'username': 'Max Mustermann',
        'city': 'Berlin'
    }
}

agent.set_context(request_data)
```

### Konversation:

```python
# Erste Nachricht (Begr√º√üung)
greeting = agent.get_response()
print(greeting)
# ‚Üí "Guten Tag! Hier spricht der Termin-Service..."

# User Antwort verarbeiten
response = agent.get_response("Ja, gerne.")
print(response)
# ‚Üí "Wunderbar! Wir suchen einen Termin f√ºr..."

# Zusammenfassung erstellen
summary = agent.get_conversation_summary()
print(summary)
# ‚Üí "Termin wurde vereinbart f√ºr Mittwoch, 14 Uhr..."
```

## üîß Integration in call_server.py

### Schritt 1: Import

```python
import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from llmcall_method.agent import AppointmentAgent
from language_output.language_output import talk
```

### Schritt 2: Global Agent

```python
# Global agent instance (wird pro Call neu erstellt)
agents = {}  # request_id -> AppointmentAgent
```

### Schritt 3: /start_conversation updaten

```python
@app.route("/start_conversation", methods=['GET', 'POST'])
def start_conversation():
    """Start the conversation with LLM context."""
    
    # Hole request_id aus Query Params
    request_id = request.args.get('request_id')
    
    if not request_id:
        # Fallback ohne Kontext
        resp = VoiceResponse()
        resp.say("Guten Tag! Hier spricht der Termin-Service.")
        resp.append(build_gather())
        return str(resp)
    
    try:
        # Lade Request-Daten aus Supabase
        request_data = load_request_from_supabase(request_id)
        
        # Erstelle neuen Agent mit Kontext
        agent = AppointmentAgent()
        agent.set_context(request_data)
        agents[request_id] = agent
        
        # Generiere erste Nachricht
        greeting = agent.get_response()
        
        # TTS
        audio_file = talk(greeting)
        
        # Twilio Response
        resp = VoiceResponse()
        resp.play(f"/audio/{audio_file}")
        resp.append(build_gather())
        return str(resp)
        
    except Exception as e:
        print(f"‚ùå Fehler: {e}")
        # Fallback
        resp = VoiceResponse()
        resp.say("Entschuldigung, es gab ein technisches Problem.")
        return str(resp)
```

### Schritt 4: /gather updaten

```python
@app.route("/gather", methods=['GET', 'POST'])
def gather():
    """Process user reply with LLM."""
    
    request_id = request.args.get('request_id')
    user_input = request.values.get('SpeechResult', '').strip()
    
    if not user_input:
        # Kein Input
        resp = VoiceResponse()
        resp.append(build_gather("Ich habe Sie nicht verstanden. K√∂nnten Sie das wiederholen?"))
        return str(resp)
    
    # Hole Agent f√ºr diesen Request
    agent = agents.get(request_id)
    
    if not agent:
        # Kein Agent gefunden - Fallback
        resp = VoiceResponse()
        resp.say("Entschuldigung, die Verbindung wurde unterbrochen.")
        return str(resp)
    
    try:
        # Generiere Antwort mit LLM
        agent_response = agent.get_response(user_input)
        
        # TTS
        audio_file = talk(agent_response)
        
        # Twilio Response
        resp = VoiceResponse()
        resp.play(f"/audio/{audio_file}")
        
        # Pr√ºfe ob Gespr√§ch beendet
        if "vielen dank" in agent_response.lower() or "auf wiedersehen" in agent_response.lower():
            # Gespr√§ch beenden
            resp.hangup()
            
            # Summary erstellen und speichern
            summary = agent.get_conversation_summary()
            save_summary_to_supabase(request_id, summary)
            
            # Agent aufr√§umen
            del agents[request_id]
        else:
            # Konversation fortsetzen
            resp.append(build_gather())
        
        return str(resp)
        
    except Exception as e:
        print(f"‚ùå Fehler: {e}")
        resp = VoiceResponse()
        resp.say("Entschuldigung, es gab ein Problem.")
        resp.hangup()
        return str(resp)
```

## üóÑÔ∏è Supabase Integration

### Request-Daten laden:

```python
from supabase import create_client
import os

supabase = create_client(
    os.getenv("SUPABASE_URL"),
    os.getenv("SUPABASE_SERVICE_KEY")  # Service Key f√ºr Backend!
)

def load_request_from_supabase(request_id: str) -> dict:
    """Lade Request-Daten aus Supabase"""
    
    # Request laden
    response = supabase.table('requests').select('*').eq('id', request_id).single().execute()
    request = response.data
    
    # User Profile laden
    profile_response = supabase.table('profiles').select('*').eq('user_id', request['user_id']).single().execute()
    profile = profile_response.data
    
    return {
        'title': request['title'],
        'description': request['description'],
        'preferred_time': request['preferred_time'],
        'user_profile': {
            'username': profile['username'],
            'city': profile['city']
        }
    }

def save_summary_to_supabase(request_id: str, summary: str):
    """Speichere Konversations-Summary in Supabase"""
    
    supabase.table('requests').update({
        'summary': summary,
        'status': 'booked',  # oder 'failed' je nach Summary
        'updated_at': 'now()'
    }).eq('id', request_id).execute()
    
    # Event erstellen
    supabase.table('events').insert({
        'request_id': request_id,
        'type': 'call_completed',
        'message': f'Call completed. {summary}'
    }).execute()
```

## üîê Environment Variables

F√ºge zu `.env` hinzu:

```env
# Gemini LLM
GOOGLE_API_KEY=your_gemini_api_key_here

# Supabase (f√ºr Backend)
SUPABASE_URL=your_supabase_url
SUPABASE_SERVICE_KEY=your_service_key_here

# ElevenLabs TTS
meinapitoken=your_elevenlabs_api_key

# Twilio
TWILIO_ACCOUNT_SID=your_account_sid
TWILIO_AUTH_TOKEN=your_auth_token
TWILIO_PHONE_NUMBER=your_twilio_number
TARGET_PHONE_NUMBER=target_number

# Public URL (ngrok/dev tunnel)
PUBLIC_BASE_URL=https://your-ngrok-url.ngrok.io
```

## üß™ Testing

### Test 1: Agent direkt testen

```bash
cd llmcall_method
python agent.py
```

### Test 2: Mit Flask Server

```bash
# Terminal 1: Flask Server starten
cd twillio
python call_server.py

# Terminal 2: Test Call
python start_call.py
```

### Test 3: End-to-End

```bash
# 1. Backend starten
./start.sh

# 2. Ngrok starten
ngrok http 5001

# 3. PUBLIC_BASE_URL in .env setzen

# 4. Request im Frontend erstellen

# 5. Call wird automatisch gestartet
```

## üìä Prompt Template Analyse

### ‚úÖ St√§rken:
- **Kontext-aware**: Nutzt Request-Daten
- **Strukturiert**: Klare Ziele und Regeln
- **Kurz**: Erzwingt pr√§zise Antworten
- **Beispiele**: Few-shot learning
- **Fallbacks**: Error Handling

### üîß Verbesserungsm√∂glichkeiten:
1. **Multi-Language**: Deutsch/Englisch je nach User
2. **Tone Anpassung**: Formell/Informell je nach Kontext
3. **Domain-specific**: Arzt/Friseur/Restaurant Templates
4. **Learning**: Feedback Loop f√ºr bessere Prompts

## üöÄ N√§chste Schritte

1. ‚úÖ AppointmentAgent implementiert
2. ‚è≥ call_server.py Integration
3. ‚è≥ Supabase Helper Functions
4. ‚è≥ Request-ID √úbergabe via Twilio
5. ‚è≥ Summary zur√ºck an Frontend
6. ‚è≥ Testing & Debugging
7. ‚è≥ Production Deployment

## üìù Notizen

- Gemini 2.0 Flash ist kostenlos und schnell
- Max 150 Tokens pro Response f√ºr kurze Antworten
- Temperature 0.7 f√ºr nat√ºrliche aber konsistente Antworten
- Conversation History auf 6 Nachrichten limitiert (Kontext-Fenster)
- Fallback-Mechanismen f√ºr Fehlerbehandlung

## üêõ Known Issues

1. **Agent Cleanup**: Agents werden nicht automatisch gel√∂scht bei abgebrochenen Calls
2. **Concurrent Calls**: Keine Limitierung f√ºr gleichzeitige Calls
3. **Error Handling**: Mehr Logging n√∂tig
4. **Testing**: Keine Unit Tests vorhanden

## üìö Weitere Ressourcen

- [Gemini API Docs](https://ai.google.dev/docs)
- [Twilio Voice Docs](https://www.twilio.com/docs/voice)
- [ElevenLabs API](https://elevenlabs.io/docs)
- [Supabase Python Client](https://supabase.com/docs/reference/python)